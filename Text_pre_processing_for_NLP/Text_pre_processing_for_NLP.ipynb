{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iPALVIKAS/Linear-Regression-using-Gradient-Descent/blob/main/Text_pre_processing_for_NLP/Text_pre_processing_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> Text Pre Processing for NLP\n",
        "\n",
        "\n",
        "<img src=\"https://images.datacamp.com/image/upload/v1669223212/Text_Mining_6eeff5cb7c.png\" width=\"800\"/>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "Y1Qe19Kv212z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Housekeeping\n",
        "1. Check that the recording is on\n",
        "2. Check audio and screenshare\n",
        "3. Share link to notebook in chat\n",
        "4. Check for light mode and readable font size"
      ],
      "metadata": {
        "id": "h00WlRlEOwuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Text Pre-processing?\n",
        "\n",
        "Text preprocessing involves **cleaning and preparing raw text data** so that it can be passed on to downstream NLP tasks, such as text analysis or model training. When conducted effectively, text preprocessing can significantly impact the performance and accuracy of NLP models.\n",
        "\n",
        "Today, we will look at some the essential steps involved in text preprocessing for NLP tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ntT6SjC3BNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Text Preprocessing is Important?\n",
        "\n",
        "Raw text data is often:\n",
        "- **Noisy**: The contents could include inconsistencies such as typos, slang, abbreviations\n",
        "- **Unstructured**: Itvexists in the form of characters, that are not machine-readable.\n",
        "\n",
        "Preprocessing helps in:\n",
        "\n",
        "- **Improving Data Quality:**\n",
        "By removing noise and irrelevant components, the processed data is made clean and consistent.\n",
        "\n",
        "- **Enhancing Model Performance:**\n",
        "When a dataset is well-preprocessed, it improves feature extraction. Which, in turn improves the performance of NLP models.\n",
        "\n",
        "- **Reducing Complexity:**\n",
        "By narrowing the dataset to relevant text elements, we can reduce the required computational overhead, and make the training of models more efficient.\n"
      ],
      "metadata": {
        "id": "-zazPxFMPLEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Today's workshop will focus on 3 use-cases for text-preprocessing, and we will work with:\n",
        "\n",
        "- **Plain Text**\n",
        "- **Web page**\n",
        "- **Pdf files**\n",
        "\n"
      ],
      "metadata": {
        "id": "GRiSXV8C0RTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plain Text: Simple Text Preprocessing Techniques:"
      ],
      "metadata": {
        "id": "IaWNRIKmvfUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Text Processing\n",
        "\n",
        "In this example, we will work on the following tasks:\n",
        "- Converting text to lowercase\n",
        "- Removing punctuation and special characters (reducing special characters)\n",
        "- Removing numbers\n",
        "\n",
        "These tasks can be carried out with basic python functions and regular expressions. More details for ReGex for NLP is available in our previous [workshop notebook](https://github.com/ua-datalab/NLP-Speech/blob/main/Introduction_to_Regular_Expressions/Introduction_to_Regular_Expressions.ipynb)"
      ],
      "metadata": {
        "id": "HiPyuBBtviHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I can't wait for the new season of my favorite show!\",\n",
        "    \"The COVID-19 pandemic has affected millions of people worldwide.\",\n",
        "    \"U.S. stocks fell on Friday after news of rising inflation.\",\n",
        "    \"Python is a great programming language ^-^ !!! ??\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "kV3gKyaQv7Th"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    return text\n",
        "\n",
        "cleaned_corpus = [clean_text(doc) for doc in corpus]\n",
        "\n",
        "print(*(item for item in cleaned_corpus), sep='\\n')"
      ],
      "metadata": {
        "id": "_9Smf5uGvHnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3515812-bb22-4d68-e497-d546d6016b62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i cant wait for the new season of my favorite show\n",
            "the covid pandemic has affected millions of people worldwide\n",
            "us stocks fell on friday after news of rising inflation\n",
            "python is a great programming language   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the NLP tasks, we will use the NLTK python library. These tasks can also be conducted using the SpaCy package.\n",
        "\n",
        "[See our previous workshop on NLP with SpaCy for more details](https://github.com/ua-datalab/NLP-Speech/blob/main/Natural_Language_Processing_Text_Mining_and_Sentiment_Analysis/Natural_Language_Processing_Text_Mining_and_Sentiment_Analysis.ipynb)."
      ],
      "metadata": {
        "id": "8ztjCs9qpVni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1tQk_iUPsCI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c1eea1-a599-4933-d679-e1d4af492643"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj7uRTttvdSq",
        "outputId": "d703f8bb-a6a8-4e42-bb8b-d1533f0c6944"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = [word_tokenize(doc) for doc in cleaned_corpus]\n",
        "print(len(tokenized_corpus))\n",
        "print(*(item for item in tokenized_corpus), sep='\\n')"
      ],
      "metadata": {
        "id": "QcgN4cvCwkIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd66e2d-867e-49ce-a372-a1be2375fbfd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "['i', 'cant', 'wait', 'for', 'the', 'new', 'season', 'of', 'my', 'favorite', 'show']\n",
            "['the', 'covid', 'pandemic', 'has', 'affected', 'millions', 'of', 'people', 'worldwide']\n",
            "['us', 'stocks', 'fell', 'on', 'friday', 'after', 'news', 'of', 'rising', 'inflation']\n",
            "['python', 'is', 'a', 'great', 'programming', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words Removal\n",
        "\n",
        "Once we have limited our dataset to words, depending on the task, we might need only meaning-carrying words in our corpus. For tasks such as topic modeling, the objective involves looking for nouns and verbs, and excluding adverbs, articles and other grammatical elements of the text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LT6sgW1QwMaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "_-Snrhpjs35a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a4fd2b-a49e-4a60-f0d9-de4a90e58f68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#create an element containing all the English stopwords:\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# iterate over each sentence, to save content words:\n",
        "filtered_corpus = [[word for word in doc if word not in stop_words] for doc in tokenized_corpus]\n",
        "print(*(item for item in filtered_corpus), sep='\\n')"
      ],
      "metadata": {
        "id": "oyeByH0gwWba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf641b8-9a94-45ba-e949-1c2603f0e1ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cant', 'wait', 'new', 'season', 'favorite', 'show']\n",
            "['covid', 'pandemic', 'affected', 'millions', 'people', 'worldwide']\n",
            "['us', 'stocks', 'fell', 'friday', 'news', 'rising', 'inflation']\n",
            "['python', 'great', 'programming', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization and Stemming\n",
        "\n",
        "While processing our documents, we may not want the model to be confused by words that have the same meaning but with different inflections (plural markers, tense markers, etc.). So we will remove the inflections and replace each token with its lemma. This uses a corpus called the WordNet, and its [NLTK library](https://www.nltk.org/howto/stem.html).\n",
        "\n",
        "Learn more about WordNet here: https://wordnet.princeton.edu/\n"
      ],
      "metadata": {
        "id": "4zT2z2mNwWym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "9Qc8nHvItWrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7844fa37-fa1f-4ec0-fbd5-7d5a8473f85d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_corpus = [[stemmer.stem(word) for word in doc] for doc in filtered_corpus]\n",
        "lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in filtered_corpus]\n",
        "print(\"Stems:\")\n",
        "print(*(item for item in stemmed_corpus), sep='\\n')\n",
        "print(\"Lemmas:\")\n",
        "print(*(item for item in lemmatized_corpus), sep='\\n')"
      ],
      "metadata": {
        "id": "HWZ2MvzKwsa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de40d7ab-73b6-4680-870a-8cfa13bcd20c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stems:\n",
            "['cant', 'wait', 'new', 'season', 'favorit', 'show']\n",
            "['covid', 'pandem', 'affect', 'million', 'peopl', 'worldwid']\n",
            "['us', 'stock', 'fell', 'friday', 'news', 'rise', 'inflat']\n",
            "['python', 'great', 'program', 'languag']\n",
            "Lemmas:\n",
            "['cant', 'wait', 'new', 'season', 'favorite', 'show']\n",
            "['covid', 'pandemic', 'affected', 'million', 'people', 'worldwide']\n",
            "['u', 'stock', 'fell', 'friday', 'news', 'rising', 'inflation']\n",
            "['python', 'great', 'programming', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In this example, after stemming, some of the words may look strange or misspelt. Note that these are just the way they are stored by NLTK. This is done so NLTK can call those entries correctly."
      ],
      "metadata": {
        "id": "Tdur60SiukLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Contractions\n",
        "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
        "\n",
        "Expanding contractions in the text.\n",
        "\n",
        "For example: ` I’ll be there within 5 minutes, won't you?`\n",
        "\n",
        "To process contractions, we will use the `contractions` library:"
      ],
      "metadata": {
        "id": "AgaVHlbTvGsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions --quiet"
      ],
      "metadata": {
        "id": "aQnLoKNqxN4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab39fa2b-10b7-44cf-a91b-4f2ec0bff677"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/289.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/289.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/118.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import textwrap\n",
        "import contractions\n",
        "# contracted text\n",
        "text = '''I'll be there within 5 min. Shouldn't you be there too?\n",
        "\t\t      I'd love to see u there my dear. It's awesome to meet new friends.\n",
        "\t\t      We've been waiting for this day for so long.\n",
        "          Aren't you leaving today?\n",
        "          '''\n",
        "\n",
        "# creating an empty list\n",
        "expanded_words = \"\"\n",
        "\n",
        "# using contractions.fix() to expand the shortened words\n",
        "for word in text.split():\n",
        "  expanded_words += contractions.fix(word) + \" \"\n",
        "\n",
        "print(\"Original text:\\n\" + textwrap.fill(text, width=70)+\"\\n\")\n",
        "print(\"Text without contractions:\\n\" +textwrap.fill(expanded_words, width=70))"
      ],
      "metadata": {
        "id": "mrmycsYIw45I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc7a6ee-71db-4028-d637-804835329f4a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "I'll be there within 5 min. Shouldn't you be there too?\n",
            "I'd love to see u there my dear. It's awesome to meet new friends.\n",
            "We've been waiting for this day for so long.           Aren't you\n",
            "leaving today?\n",
            "\n",
            "Text without contractions:\n",
            "I will be there within 5 min. Should not you be there too? I would\n",
            "love to see you there my dear. It is awesome to meet new friends. We\n",
            "have been waiting for this day for so long. Are not you leaving today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Parts of Speech (POS)\n",
        "\n",
        "In this task, we label each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc.\n",
        "\n",
        "This information is crucial for many NLP applications, including parsing, information retrieval, and text analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "uDTLDlm9a2L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "ihKnYGoXyLqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4f0d53-2c88-469d-c866-48d04d987497"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng') # More specific"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k85APoxSxbrF",
        "outputId": "03bc7293-a126-4be4-ecaf-803cf0ee3416"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the NLTK library\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Sample text\n",
        "text = \"NLTK is a powerful library for natural language processing. _^_. Also lets plan to learn this on a public datasaet avaiable in The U.S. and Europe\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "\n",
        "# Performing PoS tagging\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Displaying the PoS tagged result in separate lines\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "\n",
        "print(\"\\nPoS Tagging Result:\")\n",
        "for word, pos_tag in pos_tags:\n",
        "\tprint(f\"{word}: {pos_tag}\")\n"
      ],
      "metadata": {
        "id": "4uR3BgIHyMyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d844eb37-0974-4c1c-beb5-2720b5bd0c0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "NLTK is a powerful library for natural language processing. _^_. Also lets plan to learn this on a public datasaet avaiable in The US and Europe\n",
            "\n",
            "PoS Tagging Result:\n",
            "NLTK: NNP\n",
            "is: VBZ\n",
            "a: DT\n",
            "powerful: JJ\n",
            "library: NN\n",
            "for: IN\n",
            "natural: JJ\n",
            "language: NN\n",
            "processing: NN\n",
            ".: .\n",
            "_^_: NN\n",
            ".: .\n",
            "Also: RB\n",
            "lets: VBZ\n",
            "plan: NN\n",
            "to: TO\n",
            "learn: VB\n",
            "this: DT\n",
            "on: IN\n",
            "a: DT\n",
            "public: JJ\n",
            "datasaet: NN\n",
            "avaiable: NN\n",
            "in: IN\n",
            "The: DT\n",
            "US: NNP\n",
            "and: CC\n",
            "Europe: NNP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a breakdown of the common ones you’ll come across:\n",
        "\n",
        "1. **Noun (NN)**: A word that represents a person, place, thing, or idea.\n",
        "Examples: “cat,” “house,” “love.”\n",
        "\n",
        "2. **Verb (VB)**: A word that expresses an action or state of being.\n",
        "\n",
        "Examples: “run,” “eat,” “is.”\n",
        "\n",
        "3. **Adjective (JJ)**: A word that describes or modifies a noun.\n",
        "\n",
        "Examples: “red,” “happy,” “tall.”\n",
        "\n",
        "4. **Adverb (RB)**: A word that modifies a verb, adjective, or other adverb, often indicating manner, time, place, degree, etc.\n",
        "\n",
        "Examples: “quickly,” “very,” “here.”\n",
        "\n",
        "5. **Pronoun (PRP)**: A word that substitutes for a noun or noun phrase.\n",
        "\n",
        "Examples: “he,” “she,” “they.”\n",
        "\n",
        "6. **Preposition (IN)**: A word that shows the relationship between a noun (or pronoun) and other words in a sentence.\n",
        "\n",
        "Examples: “in,” “on,” “at.”\n",
        "\n",
        "7. **Conjunction (CC)**: A word that connects words, phrases, or clauses.\n",
        "\n",
        "Examples: “and,” “but,” “or.”\n",
        "\n",
        "8. **Interjection (UH)**: A word or phrase that expresses emotion or exclamation."
      ],
      "metadata": {
        "id": "qRruHYxDT9Ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing web pages\n",
        "\n",
        "If you get a web data dump, a considerable portion of it will be CSS adn HTML elements used for creating the front end. We need to extract the actual contents of the page. For this task, we can use the Beautiful Soup library, that can identify and process HTML tags.\n",
        "\n",
        "Often, the text in a page is stored under specific tags (such as `<body>` or `<article>`). We may want to preserve our text along with the tag it is saved under."
      ],
      "metadata": {
        "id": "GoEAeaJj632S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Extracting text from html pages using beautiful soup\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "8BWJRlVqyI3_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_html(html_content:str):\n",
        "  '''\n",
        "  searches each html tag, and returns the contents and a dictionary containing the tag name and\n",
        "  the text housed in each html tag\n",
        "  '''\n",
        "  # Initialize an empty dictionary\n",
        "  text_dict = {}\n",
        "\n",
        "  # Create a BeautifulSoup object\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "  # Extract text, split by new line and full stops.\n",
        "  # This helps improve readability of real, complex web pages\n",
        "  text = soup.get_text(separator='\\n').strip()\n",
        "\n",
        "  # Loop through all tags and extract text\n",
        "  for tag in soup.find_all(True):  # True finds all tags\n",
        "      # Get the tag name and text\n",
        "      tag_name = tag.name\n",
        "      tag_text = tag.get_text(strip=True)\n",
        "\n",
        "      # Only add to the dictionary if there's text\n",
        "      if tag_text:\n",
        "          text_dict[tag_name] = tag_contents\n",
        "\n",
        "  return text, text_dict\n"
      ],
      "metadata": {
        "id": "5Y_58zzgh14d"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple initial example with dummy HTML:\n",
        "html_doc = \"\"\"\n",
        "<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "# extract content:\n",
        "text, tag_content = extract_text_from_html(html_doc)\n",
        "# print content attached to each contentful html tag:\n",
        "print(\"html tags found: \")\n",
        "for key, value in tag_content.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\nContents of page:\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "gFkXjv4N1Wwu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "29e4ce45-af5f-42d7-bb54-6c63f8ac2070"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tag_contents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4dd87d6a6ffd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# extract content:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# print content attached to each contentful html tag:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"html tags found: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-f8c0fa878e88>\u001b[0m in \u001b[0;36mextract_text_from_html\u001b[0;34m(html_content)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;31m# Only add to the dictionary if there's text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtag_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           \u001b[0mtext_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tag_contents' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, this process removed urls, sas they are stored within HTML `<a>` tags.\n",
        "\n",
        "We can search for content housed in specific tags as well.\n"
      ],
      "metadata": {
        "id": "xaNhUk0SySzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Scrape and extract text\n",
        "\n",
        "We will use [this](https://en.wikisource.org/wiki/Moral_letters_to_Lucilius) wikipedia page, containing the letters written by Seneca."
      ],
      "metadata": {
        "id": "JY_FvL608OMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "def fetch_html(url):\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Raise an exception if the request was unsuccessful\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Return the HTML content\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "WjcPfByp8ALV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import page containing links to all of Seneca's letters\n",
        "# get web address\n",
        "src = \"https://en.wikisource.org/wiki/Moral_letters_to_Lucilius\"\n",
        "\n",
        "wiki = requests.get(src).text  # pull html as text\n",
        "# print a small chunk, to see the output:\n",
        "print(textwrap.fill(html_doc, width=70))\n"
      ],
      "metadata": {
        "id": "SaVMRQoTzIoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214f2775-4a99-44ab-e17c-9f5138229f00"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " <html><head><title>The Dormouse's story</title></head> <body>  <p\n",
            "class=\"story\">Once upon a time there were three little sisters; and\n",
            "their names were <a href=\"http://example.com/elsie\" class=\"sister\"\n",
            "id=\"link1\">Elsie</a>, <a href=\"http://example.com/lacie\"\n",
            "class=\"sister\" id=\"link2\">Lacie</a> and <a\n",
            "href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
            "and they lived at the bottom of a well.</p>  <p class=\"story\">...</p>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract plain text from the raw source code"
      ],
      "metadata": {
        "id": "b3w5qA2G9NsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a BeautifulSoup object\n",
        "soup = BeautifulSoup(wiki, 'html.parser')\n",
        "\n",
        "# Extract text, split by full stops:\n",
        "text = soup.get_text(separator='\\n').strip()\n",
        "\n",
        "print(textwrap.fill(text, width=70))"
      ],
      "metadata": {
        "id": "jf8N3cVD8fZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4524dd5c-9241-4667-b202-59f5d86dc496"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moral letters to Lucilius - Wikisource, the free online library\n",
            "Jump to content                 Main menu             Main menu   move\n",
            "to sidebar   hide                        Navigation\n",
            "Main Page Community portal Central discussion Recent changes Subject\n",
            "index Authors Random work Random author Random transcription Help\n",
            "Display Options\n",
            "Search                         Search\n",
            "Appearance                                   Donate     Create account\n",
            "Log in                   Personal tools             Donate   Create\n",
            "account   Log in                            Pages for logged out\n",
            "editors  learn more         Contributions Talk\n",
            "Moral letters to Lucilius         4 languages           العربية\n",
            "Español Français Latina     Edit links                         Page\n",
            "Source Discussion             English\n",
            "Read Edit View history                 Tools             Tools   move\n",
            "to sidebar   hide                        Actions                Read\n",
            "Edit View history                            General\n",
            "What links here Related changes Special pages Permanent link Page\n",
            "information Cite this page Get shortened URL Download QR code\n",
            "Print/export                Printable version Download EPUB Download\n",
            "MOBI Download PDF Other formats                            In other\n",
            "projects                Wikimedia Commons Wikipedia Wikidata item\n",
            "Appearance   move to sidebar   hide                     Download\n",
            "From Wikisource       ← Moral letters to Lucilius (Epistulae morales\n",
            "ad Lucilium)  ( 1917/1920/1925 ) by  Seneca ,  translated by  Richard\n",
            "Mott Gummere → sister projects :  Wikipedia article ,  Commons\n",
            "category ,  Wikidata item   A Loeb Classical Library edition; volume 1\n",
            "published 1917; volume 2 published 1920; volume 3 published 1925\n",
            "482782 Moral letters to Lucilius (Epistulae morales ad Lucilium)\n",
            "Richard Mott Gummere Seneca   Layout 2       ​   THE LOEB CLASSICAL\n",
            "LIBRARY  EDITED BY   E. CAPPS,   PH.D., LL.D. T. E. PAGE,   LITT.D. W.\n",
            "H. D. ROUSE,   LITT.D.         SENECA  AD LUCILIUM EPISTULAE MORALES\n",
            "                  ​   SENECA   AD LUCILIUM   EPISTULAE MORALES   WITH\n",
            "AN ENGLISH TRANSLATION BY   RICHARD M. GUMMERE, P H . D .   OF\n",
            "HAVERFORD COLLEGE    IN THREE VOLUMES             LONDON : WILLIAM\n",
            "HEINEMANN   NEW YORK : G. P. PUTNAM'S SONS     MCMXXV       ​  \n",
            "First Printed  1918 .     Reprinted 1925.             Printed in Great\n",
            "Britain.                         ​           CONTENTS OF VOLUME I\n",
            "PAGE         INTRODUCTION     vii                     LETTERS—\n",
            "I.     ON SAVING TIME     2        II.     ON DISCURSIVENESS IN\n",
            "READING     6        III.     ON TRUE AND FALSE FRIENDSHIP     8\n",
            "IV.     ON THE TERRORS OF DEATH     12        V.     ON THE\n",
            "PHILOSOPHER’S MEAN     20        VI.     ON SHARING KNOWLEDGE     24\n",
            "VII.     ON CROWDS     28        VIII.     ON THE PHILOSOPHER’S\n",
            "SECLUSION     36        IX.     ON PHILOSOPHY AND FRIENDSHIP     42\n",
            "X.     ON LIVING TO ONESELF     56        XI.     ON THE BLUSH OF\n",
            "MODESTY     60        XII.     ON OLD AGE     64        XIII.     ON\n",
            "GROUNDLESS FEARS     72        XIV.     ON THE REASONS FOR WITHDRAWING\n",
            "FROM THE WORLD     84        XV.     ON BRAWN AND BRAINS     94\n",
            "XVI.     ON PHILOSOPHY, THE GUIDE OF LIFE     102        XVII.     ON\n",
            "PHILOSOPHY AND RICHES     108        XVIII.     ON FESTIVALS AND\n",
            "FASTING     116        XIX.     ON WORLDLINESS AND RETIREMENT     124\n",
            "​       XX.     ON PRACTISING WHAT YOU PREACH     132        XXI.\n",
            "ON THE RENOWN WHICH MY WRITINGS WILL BRING YOU     140        XXII.\n",
            "ON THE FUTILITY OF HALF-WAY MEASURES     148        XXIII.     ON THE\n",
            "TRUE JOY WHICH COMES FROM PHILOSOPHY     158        XXIV.     ON\n",
            "DESPISING DEATH     164        XXV.     ON REFORMATION     182\n",
            "XXVI.     ON OLD AGE AND DEATH     186        XXVII.     ON THE GOOD\n",
            "WHICH ABIDES     192        XXVIII.     ON TRAVEL AS A CURE FOR\n",
            "DISCONTENT     198        XXIX.     ON THE CRITICAL CONDITION OF\n",
            "MARCELLINUS     202        XXX.     ON CONQUERING THE CONQUEROR\n",
            "210        XXXI.     ON SIREN SONGS     222        XXXII.     ON\n",
            "PROGRESS     228        XXXIII.     ON THE FUTILITY OF LEARNING MAXIMS\n",
            "232        XXXIV.     ON A PROMISING PUPIL     240        XXXV.     ON\n",
            "THE FRIENDSHIP OF KINDRED MINDS     242        XXXVI.     ON THE VALUE\n",
            "OF RETIREMENT     246        XXXVII.     ON ALLEGIANCE TO VIRTUE\n",
            "252        XXXVIII.     ON QUIET CONVERSATION     256        XXXIX.\n",
            "ON NOBLE ASPIRATIONS     258        XL.     ON THE PROPER STYLE FOR A\n",
            "PHILOSOPHER’S DISCOURSE     262        XLI.     ON THE GOD WITHIN US\n",
            "272        XLII.     ON VALUES     278        XLIII.     ON THE\n",
            "RELATIVITY OF FAME     284        XLIV.     ON PHILOSOPHY AND\n",
            "PEDIGREES     286        XLV.     ON SOPHISTICAL ARGUMENTATION     290\n",
            "​       XLVI.     ON A NEW BOOK BY LUCILIUS     298        XLVII.\n",
            "ON MASTER AND SLAVE     300        XLVIII.     ON QUIBBLING AS\n",
            "UNWORTHY OF THE PHILOSOPHER     312        XLIX.     ON THE SHORTNESS\n",
            "OF LIFE     322        L.     ON OUR BLINDNESS AND ITS CURE     330\n",
            "LI.     ON BAIAE AND MORALS     336        LII.     ON CHOOSING OUR\n",
            "TEACHERS     344        LIII.     ON THE FAULTS OF THE SPIRIT     352\n",
            "LIV.     ON ASTHMA AND DEATH     360        LV.     ON VATIA’S VILLA\n",
            "364        LVI.     ON QUIET AND STUDY     372        LVII.     ON THE\n",
            "TRIALS OF TRAVEL     382        LVIII.     ON BEING     386\n",
            "LIX.     ON PLEASURE AND JOY     408        LX.     ON HARMFUL PRAYERS\n",
            "422        LXI.     ON MEETING DEATH CHEERFULLY     424        LXII.\n",
            "ON GOOD COMPANY     426        LXIII.     ON GRIEF FOR LOST FRIENDS\n",
            "428        LXIV.     ON THE PHILOSOPHER’S TASK     438        LXV.\n",
            "ON THE FIRST CAUSE     444                     INDEX         461\n",
            "​           CONTENTS OF VOLUME II                      LETTERS—\n",
            "PAGE         LXVI.     ON VARIOUS ASPECTS OF VIRTUE     2\n",
            "LXVII.     ON ILL-HEALTH AND ENDURANCE OF SUFFERING     34\n",
            "LXVIII.     ON WISDOM AND RETIREMENT     44        LXIX.     ON REST\n",
            "AND RESTLESSNESS     52        LXX.     ON THE PROPER TIME TO SLIP THE\n",
            "CABLE     56        LXXI.     ON THE SUPREME GOOD     72        LXXII.\n",
            "ON BUSINESS AS THE ENEMY OF PHILOSOPH Y    96        LXXIII.     ON\n",
            "PHILOSOPHERS AND KINGS     104        LXXIV.     ON VIRTUE AS A REFUGE\n",
            "FROM WORLDLY DISTRACTIONS     112        LXXV.     ON THE DISEASES OF\n",
            "THE SOUL     136        LXXVI.     ON LEARNING WISDOM IN OLD AGE\n",
            "146        LXXVII.     ON TAKING ONE’S OWN LIFE     168\n",
            "LXXVIII.     ON THE HEALING POWER OF THE MIND     180        LXXIX.\n",
            "ON THE REWARDS OF SCIENTIFIC DISCOVERY     200        LXXX.     ON\n",
            "WORLDLY DECEPTIONS     212        LXXXI.     ON BENEFITS     218\n",
            "LXXXII.     ON THE NATURAL FEAR OF DEATH     240        LXXXIII.\n",
            "ON DRUNKENNESS     258        LXXXIV.     ON GATHERING IDEAS     276\n",
            "​       LXXXV.     ON SOME VAIN SYLLOGISMS     284        LXXXVI.\n",
            "ON SCIPIO’S VILLA     310        LXXXVII.     SOME ARGUMENTS IN FAVOUR\n",
            "OF THE SIMPLE LIFE     322        LXXXVIII.     ON LIBERAL AND\n",
            "VOCATIONAL STUDIES     348        LXXXIX.     ON THE PARTS OF\n",
            "PHILOSOPHY     376        XC.     ON THE PART PLAYED BY PHILOSOPHY IN\n",
            "THE PROGRESS OF MAN     394        XCI.     ON THE LESSON TO BE DRAWN\n",
            "FROM THE BURNING OF LYONS     430        XCII.     ON THE HAPPY LIFE\n",
            "446                     APPENDIX     472                     INDEX OF\n",
            "PROPER NAMES     474        ​           CONTENTS OF VOLUME III\n",
            "             LETTERS—                PAGE         XCIII.     ON THE\n",
            "QUALITY, AS CONTRASTED WITH THE LENGTH, OF LIFE     2        XCIV.\n",
            "ON THE VALUE OF ADVICE     10        XCV.     ON THE USEFULNESS OF\n",
            "BASIC PRINCIPLES     58        XCVI.     ON FACING HARDSHIPS     104\n",
            "XCVII.     ON THE DEGENERACY OF THE AGE     108        XCVIII.     ON\n",
            "THE FICKLENESS OF FORTUNE     118        XCIX.     ON CONSOLATION TO\n",
            "THE BEREAVED     128        C.     ON THE WRITINGS OF FABIANUS     148\n",
            "CI.     ON THE FUTILITY OF PLANNING AHEAD     158        CII.     ON\n",
            "THE INTIMATIONS OF OUR IMMORTALITY     168        CIII.     ON THE\n",
            "DANGERS OF ASSOCIATION WITH OUR FELLOW-MEN     186        CIV.     ON\n",
            "CARE OF HEALTH AND PEACE OF MIND     190        CV.     ON FACING THE\n",
            "WORLD WITH CONFIDENCE     212        CVI.     ON THE CORPOREALITY OF\n",
            "VIRTUE     216        CVII.     ON OBEDIENCE TO THE UNIVERSAL WILL\n",
            "222        CVIII.     ON THE APPROACHES TO PHILOSOPHY     228\n",
            "CIX.     ON THE FELLOWSHIP OF WISE MEN     254        CX.     ON TRUE\n",
            "AND FALSE RICHES     264        CXI.     ON THE VANITY OF MENTAL\n",
            "GYMNASTICS     276  ​       CXII.     ON REFORMING HARDENED SINNERS\n",
            "280        CXIII.     ON THE VITALITY OF THE SOUL AND ITS ATTRIBUTES\n",
            "282        CXIV.     ON STYLE AS A MIRROR OF CHARACTER     300\n",
            "CXV.     ON THE SUPERFICIAL BLESSINGS     318        CXVI.     ON\n",
            "SELF-CONTROL     332        CXVII.     ON REAL ETHICS AS SUPERIOR TO\n",
            "SYLLOGISTIC SUBTLETIES     336        CXVIII.     ON THE VANITY OF\n",
            "PLACE-SEEKING     360        CXIX.     ON NATURE AS OUR BEST PROVIDER\n",
            "370        CXX.     MORE ABOUT VIRTUE     380        CXXI.     ON\n",
            "INSTINCT IN ANIMALS     396        CXXII.     ON DARKNESS AS A VEIL\n",
            "FOR WICKEDNESS     410        CXXIII.     ON THE CONFLICT BETWEEN\n",
            "PLEASURE AND VIRTUE     424        CXXIV.     ON THE TRUE GOOD AS\n",
            "ATTAINED BY REASON     434                     APPENDIX A     451\n",
            "             APPENDIX B     453                     INDEX OF PROPER\n",
            "NAMES     455                     SUBJECT INDEX     459\n",
            "This work is in the  public domain  in the  United States  because it\n",
            "was published before January 1, 1930.      The longest-living author\n",
            "of this work died in 1969, so this work is in the  public domain  in\n",
            "countries and areas where the copyright term is the author's  life\n",
            "plus 55 years or less . This work may be in the  public domain  in\n",
            "countries and areas with longer native copyright terms that apply the\n",
            "rule of the shorter term  to  foreign works .                Public\n",
            "domain Public domain false false               Retrieved from \" https:\n",
            "//en.wikisource.org/w/index.php?title=Moral_letters_to_Lucilius&oldid=\n",
            "14844031 \"   Categories :  1917 works 1920 works 1925 works Works of\n",
            "uncertain date PD-old-50-US Ancient Roman philosophical works Works\n",
            "originally in Latin Hidden categories:  Borders using bthickness\n",
            "parameter Borders using color parameter Borders using align parameter\n",
            "Borders using padding parameter                This page was last\n",
            "edited on 3 February 2025, at 20:03.   Text is available under the\n",
            "Creative Commons Attribution-ShareAlike License ; additional terms may\n",
            "apply.  By using this site, you agree to the  Terms of Use  and\n",
            "Privacy Policy.       Privacy policy   About Wikisource   Disclaimers\n",
            "Code of Conduct   Developers   Statistics   Cookie statement   Mobile\n",
            "view                               Search\n",
            "Search             Moral letters to Lucilius\n",
            "4 languages       Add topic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional\n",
        "\n",
        "Try running the subtasks discussed in the previous task, to turn this text dump into input for an NLP pieline."
      ],
      "metadata": {
        "id": "gopYDJmW0HfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF files\n",
        "\n",
        "PDF files are a common way to transmit documents. However, they work more like images than a text corpus. So, we will need to extract text from them.\n",
        "\n",
        "For this task, we will use a python library that processed PDFs, and conduct the NLP tasks with NLTK."
      ],
      "metadata": {
        "id": "2sYA-Oab9pz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "FvQc7GTK-YKh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d088f2-ae27-4098-d327-802499bd4d51"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/232.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Extract names of individuals from an exerpt of a report in PDF format.\n",
        "\n",
        "Consider the names from the [Wiki entry for Adar](https://lotr.fandom.com/wiki/Adar), from the LORT universe.\n",
        "\n",
        "Every entry on the wikipages contains several names, and researching one character could turn into looking up other characters. So, in order to understand what we are getting into, it may be worth it to take stock of all the mentions on a page. :\n",
        "\n",
        "\n",
        "> Adar became known to the peoples of Middle-earth much later in the Second Age, first appearing in a large trench dug by his servant, Magrot, immediately after Arondir's failed attempt to cause a fray and escape. While Arondir is pinned down, having stabbed Magrot in the neck, Lurka orders he be brought to Adar. The \"Lord-father\" then emerges as the Orcs around him bow. He gently tends to the dying Magrot, who had sustained a mortal wound in the Elves' escape attempt, before suddenly ending his suffering with a dagger. As the rest of the Orcs leave, Adar speaks to Arondir, learning the Silvan Elf's birthplace to be in Beleriand. Adar reminisces about his days along the river Sirion, though he evades Arondir's own questions, before releasing Arondir to take a message to the Southlanders taking refuge in the Watchtower of Ostirith: that they may live if they forsake the territory and swear fealty to him. Later, as he watches one of the caged Wargs devouring fresh flesh, Adar is informed by Grugzûk that the Orc Sigil Hilt that they seek is in the watchtower.\n",
        "\n",
        "\n",
        "**What are some of the character names you can spot?**"
      ],
      "metadata": {
        "id": "xfWXj_W9CPKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional: run in case of bugs:\n",
        "# !pip install pip==24.0\n",
        "# !pip install textract --no-cache-dir"
      ],
      "metadata": {
        "id": "306473Xe_JVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the PDF file\n",
        "url = \"https://raw.githubusercontent.com/ua-datalab/NLP-Speech/main/Text_pre_processing_for_NLP/adar_pdf_example.pdf\"\n",
        "# Send a GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the response is successful\n",
        "if response.status_code == 200:\n",
        "    # Save the PDF to a file\n",
        "    with open('adar_pdf_example.pdf', 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(\"PDF downloaded successfully!\")\n",
        "else:\n",
        "    print(f\"Failed to download PDF. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "xgtHWE9js0be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a61082-86ee-456f-8378-60763bbd6e10"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the PDF\n",
        "import PyPDF2\n",
        "\n",
        "pdf_file = '/content/adar_pdf_example.pdf'\n",
        "pdfreader = PyPDF2.PdfReader(open(pdf_file, 'rb'))\n",
        "# pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
      ],
      "metadata": {
        "id": "O8WCVoFV_DXO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting characters from a PDF:"
      ],
      "metadata": {
        "id": "HALeiHlfOnDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting page 2 of the docuemnt\n",
        "pageObj = pdfreader.pages[2]\n",
        "pdf_extract = pageObj.extract_text()\n",
        "print(textwrap.fill(pdf_extract, width=90))"
      ],
      "metadata": {
        "id": "W8o-BSOvBmOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9be7fca-b6a0-4f7f-8198-c6bf4b167ebf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/9/24, 12:57Adar | The One Wiki to Rule Them All | Fandom Page 3 of\n",
            "10https://lotr.fandom.com/wiki/AdarBiographyYears of the Trees & First AgeIn the First\n",
            "Age, the Elf who would later become known as Adar walkedalongside the river Sirion in\n",
            "Beleriand, which had banks covered by miles ofsage blossoms.[1]He was one of the\n",
            "Moriondor, the thirteen Elves chosen to be corrupted byMorgoth in the Elder Days. Lured by\n",
            "the promise of power, he was led up a dark,nameless peak, chained and abandoned to hunger\n",
            "and thirst. Morgoth'sservant, Sauron, eventually appeared and ofered him red wine. He\n",
            "drank thewine, forever changing his nature.[2] His nature was later perceived by\n",
            "Galadrielafter she briegy captured him in the Second Age.[3] Subsequent generations ofthe\n",
            "newly bred race of Orcs considered him to be their \"father\", and followedhim\n",
            "willingly.Second AgeAfter Morgoth's defeat,Adar entered theservice of Sauron\n",
            "afteranswering his call tofollow him to thefortress Dúrnost,becoming his lieutenant.[4] At\n",
            "Drst he helped his new master in his goal to bringthe lands of Middle-earth \"into perfect\n",
            "order\", but he lost faith in Sauron aftersacriDcing \"enough of [his] children for\n",
            "[Sauron's] aspirations\". During thecoronation of Sauron in Dúrnost, Adar initially stood\n",
            "by his master's side andeven hailed him as the new Dark Lord. Trusted with placing\n",
            "Morgoth's crownupon Sauron's head, Adar instead used it to stab him. Adar then looked on\n",
            "ashis \"children\" joined his attack on the weakened Sauron. Wounded, the DarkLord's body\n",
            "disintegrated,[4] leading Adar to believe for centuries afterwardthat he had slain Sauron.\n",
            "For many years, Adar remained the leader of a largeportion of surviving Orcs, though in\n",
            "secret.Adar became known tothe peoples of Middle-earth much later in theSecond Age,\n",
            "Drstappearing in a largetrench dug by hisservant, Magrot,immediately afterArondir's failed\n",
            "attempt to cause a fray and escape. While Arondir is pinneddown, having stabbed Magrot in\n",
            "the neck, Lurka orders he be brought to Adar.The \"Lord-father\" then emerges as the Orcs\n",
            "around him bow.[1] He gently tendsto the dying Magrot, who had sustained a mortal wound in\n",
            "the Elves' escapeattempt, before suddenly ending his sufering with a dagger. As the rest\n",
            "of theOrcs leave, Adar speaks to Arondir, learning the Silvan Elf's birthplace to be\n",
            "inBeleriand. Adar reminisces about his days along the river Sirion, though heevades\n",
            "Arondir's own questions, before releasing Arondir to take a message tothe Southlanders\n",
            "taking refuge in the Watchtower of Ostirith: that they maylive if they forsake the\n",
            "territory and swear fealty to him. Later, as he watchesone of the caged Wargs devouring\n",
            "fresh gesh, Adar is informed by Grugzûk thatthe Orc Sigil Hilt that they seek is in the\n",
            "watchtower.[5] Adar turns on his master Adar speaks to Arondir START AWIKI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have processed the PDF, let us fix some OCR issues that could impact our search for people. You may notice that the OCR recognition omits a considerable number of spaces.\n",
        "\n",
        "While there are advanced tools for typo detections, because this is fantasy fiction, spell-checkers would also target nouns it doesn't recognize. So, we will stick to regular expressions for cleanup. Since we are only interested in proper nouns, a splitting capitalized words is all we need to do."
      ],
      "metadata": {
        "id": "pHy1FB-uMRS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we re-reun our pipeline for text processing:\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # We will keep the following characters, and remove the special characters:\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!\\'?()~@#$%^&*-+=]', '', text)\n",
        "    #Look for capitalized letters in the middle of words, and split the word\n",
        "    # Include in case the OCR has errors:\n",
        "    # text = re.sub(r'(?<!^)(?=[A-Z])', ' ', text)\n",
        "    return text\n",
        "\n",
        "cleaned_corpus = clean_text(pdf_extract)\n",
        "print(textwrap.fill(cleaned_corpus, width=90))"
      ],
      "metadata": {
        "id": "hT_dOPX144Lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88725247-ba89-4df6-e467-3afb6bd22d77"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", Adar  The One Wiki to Rule Them All  Fandom Page  of\n",
            "httpslotr.fandom.comwikiAdarBiographyYears of the Trees & First AgeIn the First Age, the\n",
            "Elf who would later become known as Adar walkedalongside the river Sirion in Beleriand,\n",
            "which had banks covered by miles ofsage blossoms.He was one of the Moriondor, the thirteen\n",
            "Elves chosen to be corrupted byMorgoth in the Elder Days. Lured by the promise of power,\n",
            "he was led up a dark,nameless peak, chained and abandoned to hunger and thirst.\n",
            "Morgoth'sservant, Sauron, eventually appeared and ofered him red wine. He drank thewine,\n",
            "forever changing his nature. His nature was later perceived by Galadrielafter she briegy\n",
            "captured him in the Second Age. Subsequent generations ofthe newly bred race of Orcs\n",
            "considered him to be their father, and followedhim willingly.Second AgeAfter Morgoth's\n",
            "defeat,Adar entered theservice of Sauron afteranswering his call tofollow him to\n",
            "thefortress Drnost,becoming his lieutenant. At Drst he helped his new master in his goal\n",
            "to bringthe lands of Middleearth into perfect order, but he lost faith in Sauron\n",
            "aftersacriDcing enough of his children for Sauron's aspirations. During thecoronation of\n",
            "Sauron in Drnost, Adar initially stood by his master's side andeven hailed him as the new\n",
            "Dark Lord. Trusted with placing Morgoth's crownupon Sauron's head, Adar instead used it to\n",
            "stab him. Adar then looked on ashis children joined his attack on the weakened Sauron.\n",
            "Wounded, the DarkLord's body disintegrated, leading Adar to believe for centuries\n",
            "afterwardthat he had slain Sauron. For many years, Adar remained the leader of a\n",
            "largeportion of surviving Orcs, though in secret.Adar became known tothe peoples of\n",
            "Middleearth much later in theSecond Age, Drstappearing in a largetrench dug by hisservant,\n",
            "Magrot,immediately afterArondir's failed attempt to cause a fray and escape. While Arondir\n",
            "is pinneddown, having stabbed Magrot in the neck, Lurka orders he be brought to Adar.The\n",
            "Lordfather then emerges as the Orcs around him bow. He gently tendsto the dying Magrot,\n",
            "who had sustained a mortal wound in the Elves' escapeattempt, before suddenly ending his\n",
            "sufering with a dagger. As the rest of theOrcs leave, Adar speaks to Arondir, learning the\n",
            "Silvan Elf's birthplace to be inBeleriand. Adar reminisces about his days along the river\n",
            "Sirion, though heevades Arondir's own questions, before releasing Arondir to take a\n",
            "message tothe Southlanders taking refuge in the Watchtower of Ostirith that they maylive\n",
            "if they forsake the territory and swear fealty to him. Later, as he watchesone of the\n",
            "caged Wargs devouring fresh gesh, Adar is informed by Grugzk thatthe Orc Sigil Hilt that\n",
            "they seek is in the watchtower. Adar turns on his master Adar speaks to Arondir START\n",
            "AWIKI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilizing the NLTK pipeline\n",
        "\n",
        "Next, we will run the NLTK pipeline to extract names entities. The `ne_chunk`[link text](https:// [link text](https://)) function adds a lot of information to tokens."
      ],
      "metadata": {
        "id": "rEUx45VpOxIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "import re\n"
      ],
      "metadata": {
        "id": "YtX2egm858h4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ea7f70-2333-4766-a182-9ce8e74be770"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "id": "m_Y2EytA1qJw",
        "outputId": "ea53257b-721e-4ff7-afbd-b075f56c5bc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the cleaned corpus\n",
        "def clean_nltk(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  tagged = pos_tag(tokens)\n",
        "  named_entities = ne_chunk(tagged)\n",
        "  return named_entities\n",
        "\n",
        "# Parse and tag each sentence, and extract named entities:\n",
        "nltk_results = clean_nltk(cleaned_corpus)\n",
        "\n",
        "print(nltk_results)\n"
      ],
      "metadata": {
        "id": "7B3FGfSB_V7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a26b42-e5ac-40bf-e227-a0d3abf611b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  ,/,\n",
            "  Adar/NNP\n",
            "  The/DT\n",
            "  (ORGANIZATION One/NNP Wiki/NNP)\n",
            "  to/TO\n",
            "  Rule/VB\n",
            "  Them/NNP\n",
            "  All/NNP\n",
            "  Fandom/NNP\n",
            "  Page/NNP\n",
            "  of/IN\n",
            "  httpslotr.fandom.comwikiAdarBiographyYears/NNS\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Trees/NNP)\n",
            "  &/CC\n",
            "  (PERSON First/NNP)\n",
            "  AgeIn/NNP\n",
            "  the/DT\n",
            "  (ORGANIZATION First/NNP Age/NNP)\n",
            "  ,/,\n",
            "  the/DT\n",
            "  Elf/NNP\n",
            "  who/WP\n",
            "  would/MD\n",
            "  later/RB\n",
            "  become/VB\n",
            "  known/VBN\n",
            "  as/IN\n",
            "  (PERSON Adar/NNP)\n",
            "  walkedalongside/IN\n",
            "  the/DT\n",
            "  river/NN\n",
            "  Sirion/NNP\n",
            "  in/IN\n",
            "  (GPE Beleriand/NNP)\n",
            "  ,/,\n",
            "  which/WDT\n",
            "  had/VBD\n",
            "  banks/NNS\n",
            "  covered/VBN\n",
            "  by/IN\n",
            "  miles/NNS\n",
            "  ofsage/RB\n",
            "  blossoms.He/NN\n",
            "  was/VBD\n",
            "  one/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Moriondor/NNP)\n",
            "  ,/,\n",
            "  the/DT\n",
            "  thirteen/JJ\n",
            "  Elves/NNP\n",
            "  chosen/NN\n",
            "  to/TO\n",
            "  be/VB\n",
            "  corrupted/VBN\n",
            "  byMorgoth/DT\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Elder/NNP Days/NNP)\n",
            "  ./.\n",
            "  Lured/VBN\n",
            "  by/IN\n",
            "  the/DT\n",
            "  promise/NN\n",
            "  of/IN\n",
            "  power/NN\n",
            "  ,/,\n",
            "  he/PRP\n",
            "  was/VBD\n",
            "  led/VBN\n",
            "  up/RP\n",
            "  a/DT\n",
            "  dark/NN\n",
            "  ,/,\n",
            "  nameless/JJ\n",
            "  peak/NN\n",
            "  ,/,\n",
            "  chained/VBN\n",
            "  and/CC\n",
            "  abandoned/VBN\n",
            "  to/TO\n",
            "  hunger/JJR\n",
            "  and/CC\n",
            "  thirst/RB\n",
            "  ./.\n",
            "  Morgoth'sservant/NNP\n",
            "  ,/,\n",
            "  (GPE Sauron/NNP)\n",
            "  ,/,\n",
            "  eventually/RB\n",
            "  appeared/VBD\n",
            "  and/CC\n",
            "  ofered/VBD\n",
            "  him/PRP\n",
            "  red/JJ\n",
            "  wine/NN\n",
            "  ./.\n",
            "  He/PRP\n",
            "  drank/VBD\n",
            "  thewine/NN\n",
            "  ,/,\n",
            "  forever/RB\n",
            "  changing/VBG\n",
            "  his/PRP$\n",
            "  nature/NN\n",
            "  ./.\n",
            "  His/PRP$\n",
            "  nature/NN\n",
            "  was/VBD\n",
            "  later/RB\n",
            "  perceived/VBN\n",
            "  by/IN\n",
            "  Galadrielafter/NNP\n",
            "  she/PRP\n",
            "  briegy/VBD\n",
            "  captured/VBD\n",
            "  him/PRP\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Second/JJ Age/NNP)\n",
            "  ./.\n",
            "  Subsequent/JJ\n",
            "  generations/NNS\n",
            "  ofthe/VBP\n",
            "  newly/RB\n",
            "  bred/VBN\n",
            "  race/NN\n",
            "  of/IN\n",
            "  (PERSON Orcs/NNP)\n",
            "  considered/VBD\n",
            "  him/PRP\n",
            "  to/TO\n",
            "  be/VB\n",
            "  their/PRP$\n",
            "  father/NN\n",
            "  ,/,\n",
            "  and/CC\n",
            "  followedhim/JJ\n",
            "  willingly.Second/NN\n",
            "  (ORGANIZATION AgeAfter/NNP Morgoth/NNP)\n",
            "  's/POS\n",
            "  defeat/NN\n",
            "  ,/,\n",
            "  (PERSON Adar/NNP)\n",
            "  entered/VBD\n",
            "  theservice/NN\n",
            "  of/IN\n",
            "  (GPE Sauron/NNP)\n",
            "  afteranswering/VBG\n",
            "  his/PRP$\n",
            "  call/NN\n",
            "  tofollow/NN\n",
            "  him/PRP\n",
            "  to/TO\n",
            "  thefortress/VB\n",
            "  (PERSON Drnost/NNP)\n",
            "  ,/,\n",
            "  becoming/VBG\n",
            "  his/PRP$\n",
            "  lieutenant/NN\n",
            "  ./.\n",
            "  At/IN\n",
            "  Drst/NNP\n",
            "  he/PRP\n",
            "  helped/VBD\n",
            "  his/PRP$\n",
            "  new/JJ\n",
            "  master/NN\n",
            "  in/IN\n",
            "  his/PRP$\n",
            "  goal/NN\n",
            "  to/TO\n",
            "  bringthe/VB\n",
            "  lands/NNS\n",
            "  of/IN\n",
            "  (GPE Middleearth/NNP)\n",
            "  into/IN\n",
            "  perfect/JJ\n",
            "  order/NN\n",
            "  ,/,\n",
            "  but/CC\n",
            "  he/PRP\n",
            "  lost/VBD\n",
            "  faith/NN\n",
            "  in/IN\n",
            "  (GPE Sauron/NNP)\n",
            "  aftersacriDcing/VBG\n",
            "  enough/NN\n",
            "  of/IN\n",
            "  his/PRP$\n",
            "  children/NNS\n",
            "  for/IN\n",
            "  (PERSON Sauron/NNP)\n",
            "  's/POS\n",
            "  aspirations/NNS\n",
            "  ./.\n",
            "  During/IN\n",
            "  thecoronation/NN\n",
            "  of/IN\n",
            "  (GPE Sauron/NNP)\n",
            "  in/IN\n",
            "  (GPE Drnost/NNP)\n",
            "  ,/,\n",
            "  (PERSON Adar/NNP)\n",
            "  initially/RB\n",
            "  stood/VBD\n",
            "  by/IN\n",
            "  his/PRP$\n",
            "  master/NN\n",
            "  's/POS\n",
            "  side/NN\n",
            "  andeven/RB\n",
            "  hailed/VBD\n",
            "  him/PRP\n",
            "  as/IN\n",
            "  the/DT\n",
            "  new/JJ\n",
            "  (PERSON Dark/NNP Lord/NNP)\n",
            "  ./.\n",
            "  Trusted/VBN\n",
            "  with/IN\n",
            "  placing/VBG\n",
            "  (PERSON Morgoth/NNP)\n",
            "  's/POS\n",
            "  crownupon/NN\n",
            "  (PERSON Sauron/NNP)\n",
            "  's/POS\n",
            "  head/NN\n",
            "  ,/,\n",
            "  (PERSON Adar/NNP)\n",
            "  instead/RB\n",
            "  used/VBD\n",
            "  it/PRP\n",
            "  to/TO\n",
            "  stab/VB\n",
            "  him/PRP\n",
            "  ./.\n",
            "  (PERSON Adar/NNP)\n",
            "  then/RB\n",
            "  looked/VBD\n",
            "  on/IN\n",
            "  ashis/JJ\n",
            "  children/NNS\n",
            "  joined/VBD\n",
            "  his/PRP$\n",
            "  attack/NN\n",
            "  on/IN\n",
            "  the/DT\n",
            "  weakened/VBN\n",
            "  (GPE Sauron/NNP)\n",
            "  ./.\n",
            "  Wounded/VBD\n",
            "  ,/,\n",
            "  the/DT\n",
            "  (ORGANIZATION DarkLord/NNP)\n",
            "  's/POS\n",
            "  body/NN\n",
            "  disintegrated/VBD\n",
            "  ,/,\n",
            "  leading/VBG\n",
            "  (PERSON Adar/NNP)\n",
            "  to/TO\n",
            "  believe/VB\n",
            "  for/IN\n",
            "  centuries/NNS\n",
            "  afterwardthat/IN\n",
            "  he/PRP\n",
            "  had/VBD\n",
            "  slain/VBN\n",
            "  (GPE Sauron/NNP)\n",
            "  ./.\n",
            "  For/IN\n",
            "  many/JJ\n",
            "  years/NNS\n",
            "  ,/,\n",
            "  (PERSON Adar/NNP)\n",
            "  remained/VBD\n",
            "  the/DT\n",
            "  leader/NN\n",
            "  of/IN\n",
            "  a/DT\n",
            "  largeportion/NN\n",
            "  of/IN\n",
            "  surviving/VBG\n",
            "  (PERSON Orcs/NNP)\n",
            "  ,/,\n",
            "  though/IN\n",
            "  in/IN\n",
            "  secret.Adar/NN\n",
            "  became/VBD\n",
            "  known/VBN\n",
            "  tothe/JJ\n",
            "  peoples/NNS\n",
            "  of/IN\n",
            "  (GPE Middleearth/NNP)\n",
            "  much/JJ\n",
            "  later/RBR\n",
            "  in/IN\n",
            "  theSecond/NN\n",
            "  Age/NNP\n",
            "  ,/,\n",
            "  Drstappearing/NNP\n",
            "  in/IN\n",
            "  a/DT\n",
            "  largetrench/JJ\n",
            "  dug/NN\n",
            "  by/IN\n",
            "  hisservant/NN\n",
            "  ,/,\n",
            "  (PERSON Magrot/NNP)\n",
            "  ,/,\n",
            "  immediately/RB\n",
            "  afterArondir/VBP\n",
            "  's/POS\n",
            "  failed/JJ\n",
            "  attempt/NN\n",
            "  to/TO\n",
            "  cause/VB\n",
            "  a/DT\n",
            "  fray/NN\n",
            "  and/CC\n",
            "  escape/NN\n",
            "  ./.\n",
            "  While/IN\n",
            "  (PERSON Arondir/NNP)\n",
            "  is/VBZ\n",
            "  pinneddown/VBN\n",
            "  ,/,\n",
            "  having/VBG\n",
            "  stabbed/VBN\n",
            "  (PERSON Magrot/NNP)\n",
            "  in/IN\n",
            "  the/DT\n",
            "  neck/NN\n",
            "  ,/,\n",
            "  (GPE Lurka/NNP)\n",
            "  orders/NNS\n",
            "  he/PRP\n",
            "  be/VB\n",
            "  brought/VBN\n",
            "  to/TO\n",
            "  Adar.The/NNP\n",
            "  Lordfather/NNP\n",
            "  then/RB\n",
            "  emerges/VBZ\n",
            "  as/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Orcs/NNP)\n",
            "  around/IN\n",
            "  him/PRP\n",
            "  bow/RB\n",
            "  ./.\n",
            "  He/PRP\n",
            "  gently/RB\n",
            "  tendsto/VBZ\n",
            "  the/DT\n",
            "  dying/VBG\n",
            "  (PERSON Magrot/NNP)\n",
            "  ,/,\n",
            "  who/WP\n",
            "  had/VBD\n",
            "  sustained/VBN\n",
            "  a/DT\n",
            "  mortal/NN\n",
            "  wound/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Elves/NNP)\n",
            "  '/POS\n",
            "  escapeattempt/NN\n",
            "  ,/,\n",
            "  before/IN\n",
            "  suddenly/RB\n",
            "  ending/VBG\n",
            "  his/PRP$\n",
            "  sufering/VBG\n",
            "  with/IN\n",
            "  a/DT\n",
            "  dagger/NN\n",
            "  ./.\n",
            "  As/IN\n",
            "  the/DT\n",
            "  rest/NN\n",
            "  of/IN\n",
            "  theOrcs/NNS\n",
            "  leave/VBP\n",
            "  ,/,\n",
            "  (PERSON Adar/NNP)\n",
            "  speaks/VBZ\n",
            "  to/TO\n",
            "  (GPE Arondir/NNP)\n",
            "  ,/,\n",
            "  learning/VBG\n",
            "  the/DT\n",
            "  (ORGANIZATION Silvan/NNP Elf/NNP)\n",
            "  's/POS\n",
            "  birthplace/NN\n",
            "  to/TO\n",
            "  be/VB\n",
            "  (ORGANIZATION inBeleriand/NN)\n",
            "  ./.\n",
            "  (PERSON Adar/NNP)\n",
            "  reminisces/VBZ\n",
            "  about/IN\n",
            "  his/PRP$\n",
            "  days/NNS\n",
            "  along/IN\n",
            "  the/DT\n",
            "  river/NN\n",
            "  (PERSON Sirion/NNP)\n",
            "  ,/,\n",
            "  though/IN\n",
            "  heevades/NNS\n",
            "  (PERSON Arondir/NNP)\n",
            "  's/POS\n",
            "  own/JJ\n",
            "  questions/NNS\n",
            "  ,/,\n",
            "  before/IN\n",
            "  releasing/VBG\n",
            "  (PERSON Arondir/NNP)\n",
            "  to/TO\n",
            "  take/VB\n",
            "  a/DT\n",
            "  message/NN\n",
            "  tothe/NN\n",
            "  Southlanders/NNP\n",
            "  taking/VBG\n",
            "  refuge/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION Watchtower/NNP)\n",
            "  of/IN\n",
            "  (GPE Ostirith/NNP)\n",
            "  that/IN\n",
            "  they/PRP\n",
            "  maylive/VBP\n",
            "  if/IN\n",
            "  they/PRP\n",
            "  forsake/VBP\n",
            "  the/DT\n",
            "  territory/NN\n",
            "  and/CC\n",
            "  swear/VB\n",
            "  fealty/NN\n",
            "  to/TO\n",
            "  him/PRP\n",
            "  ./.\n",
            "  Later/RB\n",
            "  ,/,\n",
            "  as/IN\n",
            "  he/PRP\n",
            "  watchesone/CD\n",
            "  of/IN\n",
            "  the/DT\n",
            "  caged/JJ\n",
            "  Wargs/NNP\n",
            "  devouring/VBG\n",
            "  fresh/JJ\n",
            "  gesh/NN\n",
            "  ,/,\n",
            "  (PERSON Adar/NNP)\n",
            "  is/VBZ\n",
            "  informed/VBN\n",
            "  by/IN\n",
            "  (PERSON Grugzk/NNP)\n",
            "  thatthe/NN\n",
            "  (PERSON Orc/NNP Sigil/NNP Hilt/NNP)\n",
            "  that/IN\n",
            "  they/PRP\n",
            "  seek/VBP\n",
            "  is/VBZ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  watchtower/NN\n",
            "  ./.\n",
            "  (PERSON Adar/NNP)\n",
            "  turns/VBZ\n",
            "  on/IN\n",
            "  his/PRP$\n",
            "  master/NN\n",
            "  (PERSON Adar/NNP)\n",
            "  speaks/VBZ\n",
            "  to/TO\n",
            "  (PERSON Arondir/NNP START/NNP)\n",
            "  AWIKI/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will only need the contents of the Tree item for each named entity, which provides all the words\n",
        "# clustered for each NE label, along with their POS tags\n",
        "# Learn more about this module here: https://www.nltk.org/api/nltk.tree.tree.html#nltk.tree.tree.Tree\n",
        "named_entities = [ne for ne in nltk_results if type(ne) == Tree]\n",
        "print(*(ne for ne in named_entities[:20]), sep='\\n')\n",
        "\n",
        "#  Try and see what is excluded from this search.\n",
        "# You will see POS tags, but no NE labels:\n",
        "# not_named_entities = [nne for nne in nltk_results if type(nne) != Tree]\n",
        "# print(*(ne for ne in not_named_entities[:20]), sep='\\n')"
      ],
      "metadata": {
        "id": "cK3Yh7S4I79t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd3cb14-88bb-4ab2-8d39-4b2054d26ec0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ORGANIZATION One/NNP Wiki/NNP)\n",
            "(ORGANIZATION Trees/NNP)\n",
            "(PERSON First/NNP)\n",
            "(ORGANIZATION First/NNP Age/NNP)\n",
            "(PERSON Adar/NNP)\n",
            "(GPE Beleriand/NNP)\n",
            "(ORGANIZATION Moriondor/NNP)\n",
            "(ORGANIZATION Elder/NNP Days/NNP)\n",
            "(GPE Sauron/NNP)\n",
            "(ORGANIZATION Second/JJ Age/NNP)\n",
            "(PERSON Orcs/NNP)\n",
            "(ORGANIZATION AgeAfter/NNP Morgoth/NNP)\n",
            "(PERSON Adar/NNP)\n",
            "(GPE Sauron/NNP)\n",
            "(PERSON Drnost/NNP)\n",
            "(GPE Middleearth/NNP)\n",
            "(GPE Sauron/NNP)\n",
            "(PERSON Sauron/NNP)\n",
            "(GPE Sauron/NNP)\n",
            "(GPE Drnost/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Person names from the list of NERs"
      ],
      "metadata": {
        "id": "1Driv5T-O_oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice that the label \"PERSON\" is the one relevant to our search. Because this is a fantasy universe, some people have also been labelled as \"GBE\", or geographical entities. For the purposes of this exercise, we will only extract named entities with the label \"PERSON\"\n",
        "\n",
        "Try this code with some real-world text and compare the results!"
      ],
      "metadata": {
        "id": "VM9w2iWDQc2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we will only keep the Named Entities with the label \"PERSON\"\n",
        "for ne in named_entities:\n",
        "    if ne.label() == 'PERSON':\n",
        "        # join all the nouns housed in the label:\n",
        "        name = ''\n",
        "        for leaf in ne.leaves():\n",
        "            name += leaf[0] + ' '\n",
        "        print ('Type: ', ne.label(), 'Name: ', name)"
      ],
      "metadata": {
        "id": "5m94ZCA6dlxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607e80a9-8b09-446c-e9e2-9aba54635f84"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type:  PERSON Name:  First \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Orcs \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Drnost \n",
            "Type:  PERSON Name:  Sauron \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Dark Lord \n",
            "Type:  PERSON Name:  Morgoth \n",
            "Type:  PERSON Name:  Sauron \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Orcs \n",
            "Type:  PERSON Name:  Magrot \n",
            "Type:  PERSON Name:  Arondir \n",
            "Type:  PERSON Name:  Magrot \n",
            "Type:  PERSON Name:  Magrot \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Sirion \n",
            "Type:  PERSON Name:  Arondir \n",
            "Type:  PERSON Name:  Arondir \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Grugzk \n",
            "Type:  PERSON Name:  Orc Sigil Hilt \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Adar \n",
            "Type:  PERSON Name:  Arondir START \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final thoughts\n",
        "\n",
        "In this task, we worked on some simple text pre-processing steps for text extracted from different sources. I used the NLTK took for this purpose. We encountered different kinds of noise for different sources, and needed different approaches and additional steps, if the complexity of the task was higher. When we moved to a fantsy universe, our model did a fairly good job, but faced issues if the text pre-processing had errors (such as incorrect POS tagging). The purpose of showing such a complex task was to examine how important text pre-processing tools are for other downstream tasks.\n",
        "\n",
        "As this is a GIGO situation, set ups with language models trained on more data (potentially Spacy or even an LLM) coudl do an even better job, or be able to handle noisy data. We demonstrated some lightweight setups for fast processing. Try out this code with other pre-processing tools and tell us about your experience!"
      ],
      "metadata": {
        "id": "kAokU70KfHqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources and References\n",
        "\n",
        "- https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/\n",
        "- https://www.xbyte.io/how-to-do-web-scraping-and-pre-processing-for-nlp-using-python/\n",
        "- https://ydv-poonam.medium.com/how-to-extract-text-from-a-pdf-nlp-b6409422cfd2\n",
        "- https://unbiased-coder.com/extract-names-python-nltk/\n",
        "- https://towardsdatascience.com/elegant-text-pre-processing-with-nltk-in-sklearn-pipeline-d6fe18b91eb8"
      ],
      "metadata": {
        "id": "PDEUMizW96Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Housekeeping\n",
        "1. Turn off screenshare\n",
        "2. Stop recording\n",
        "3. Make the next instructor Zoom host\n"
      ],
      "metadata": {
        "id": "6JGOVuLeInkx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GeUyELRSIwT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}